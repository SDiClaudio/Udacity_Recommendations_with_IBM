# Udacity_Recommendations_with_IBM 

### Description
For this project I analyzed the interactions that users have with articles on the IBM Watson Studio platform, and made recommendations to them about new articles I think they will like.
The project is divided into the following tasks:

I. Exploratory Data Analysis

Before making recommendations of any kind, I explored the data I am working with for the project. There are some basic, required questions left by the Udacity team to be answered about the data throughout the rest of the notebook.

II. Rank Based Recommendations

To get started in building recommendations,I first found the most popular articles simply based on the most interactions. Since there are no ratings for any of the articles, it is easy to assume the articles with the most interactions are the most popular. These are then the articles I might recommend to new users (or anyone depending on what I know about them).

III. User-User Based Collaborative Filtering

In order to build better recommendations for the users of IBM's platform, I looked at users that are similar in terms of the items they have interacted with. These items could then be recommended to the similar users. This was a step in the right direction towards more personal recommendations for the users.

IV. Matrix Factorization

Finally, I completed a machine learning approach to building recommendations. Using the user-item interactions, you will build out a matrix decomposition. Using the decomposition, I got an idea of how well you can predict new articles an individual might interact with (spoiler alert - it isn't great).


### Files

- project_test.py: this is the script used to test my answwers throughout the notebook.
- articles_community.csv: data about the contenct of each article
- user_item_interactions.csv: data regarding the interaction between each article and users

### Libraries
- Pandas
- Numpy
- Matplotlib.pyplot
- pickle

### Wrangling and Cleaning

There are 17 NULL in the user_item_interactions.csv file.These NULLs have interactions with various articles, therefore I considered those interactions coming from the same "unknown" user for the puropose of the project.

The articles_community.csv contains some duplicated article_id entry which I removed.

### EDA

Following the Udacity questions I found those insights:

- 50% of individuals have 3 or fewer interactions
- The total number of user-article interactions in the dataset is 45993
- The maximum number of user-article interactions by any 1 user is 364
- The most viewed article in the dataset was viewed 937 times
- The article_id of the most viewed article is 1429.0
- The number of unique articles that have at least 1 rating 714
- The number of unique users in the dataset is 5148
- The number of unique articles on the IBM platform: 1051

### Modeling
After answering the questions by creating the appropriate functions, I moved to the ML modelling task.
In this situation, I used Singular Value Decomposition from Numpy on the user-item matrix, since the latter doesn't contain any missing values.
Then, I measured the error generated by selecting different number of latent features.

### Conclusions
The model accuracy decreases as the number of latent features increases, probabily due to the small size of the test set.
Therefore, more data is needed for a better model performance.
